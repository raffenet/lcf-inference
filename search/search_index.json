{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Aegis: HPC LLM Instance Launcher","text":"<p>Aegis automates launching configurable numbers of vLLM inference instances on HPC clusters. It handles PBS job generation, model weight staging via MPI broadcast, and per-instance orchestration.</p> <p>Currently targets Aurora (PBS). Frontier/Slurm support is planned.</p>"},{"location":"#quick-install","title":"Quick Install","text":"<pre><code>pip install .\n</code></pre> <p>For development:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"#how-it-works","title":"How It Works","text":"<ol> <li><code>aegis submit</code> renders a PBS batch script from a Jinja2 template and submits it via <code>qsub</code>. The generated job script calls <code>aegis launch</code> inside the allocation.</li> <li><code>aegis launch</code> runs inside a PBS allocation. It optionally stages model weights to local storage using MPI broadcast, then launches one <code>vllm serve</code> process per instance on the assigned nodes.</li> <li><code>aegis bench</code> benchmarks running instances in parallel via <code>mpiexec</code> and aggregates results into CSV.</li> <li><code>aegis shutdown</code> tears down instances by killing vLLM processes on nodes and/or cancelling PBS jobs.</li> </ol>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>src/aegis/\n\u251c\u2500\u2500 cli.py              # CLI entry point (argparse)\n\u251c\u2500\u2500 config.py           # Config file loading + merging with CLI args\n\u251c\u2500\u2500 scheduler.py        # PBS job generation and submission\n\u251c\u2500\u2500 launcher.py         # Core orchestration: stage weights, launch instances\n\u2514\u2500\u2500 templates/\n    \u251c\u2500\u2500 pbs_job.sh.j2   # Jinja2 template for PBS batch script\n    \u2514\u2500\u2500 instance.sh.j2  # Jinja2 template for per-node vLLM launch script\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started \u2014 Installation and first launch walkthrough</li> <li>CLI Reference \u2014 All commands and flags</li> <li>Configuration \u2014 YAML config format and all options</li> <li>Platforms \u2014 Platform-specific setup guides</li> </ul>"},{"location":"cli/","title":"CLI Reference","text":"<p>Aegis provides five subcommands: <code>submit</code>, <code>launch</code>, <code>registry</code>, <code>bench</code>, and <code>shutdown</code>.</p>"},{"location":"cli/#aegis-submit","title":"<code>aegis submit</code>","text":"<p>Generate and submit a PBS batch job.</p> <pre><code>aegis submit --config config.yaml\n</code></pre>"},{"location":"cli/#submit-specific-flags","title":"Submit-specific flags","text":"Flag Type Description <code>--dry-run</code> flag Print the generated PBS script without submitting <code>--aegis-env</code> <code>str</code> Path to a conda environment containing the aegis package <code>--wait</code> flag Block until instances are healthy and the endpoints file is written <code>--remote</code> <code>str</code> Submit via SSH to a remote login node (e.g., <code>user@aurora.alcf.anl.gov</code>)"},{"location":"cli/#-wait","title":"<code>--wait</code>","text":"<p>By default <code>aegis submit</code> exits immediately after <code>qsub</code> succeeds. Pass <code>--wait</code> to block until the endpoints file appears:</p> <pre><code>aegis submit --config config.yaml --wait\n</code></pre> <p>The command polls the PBS job with <code>qstat</code> and watches for the configured <code>endpoints_file</code>. Once the file is found and non-empty the endpoints are printed to stdout and the command exits. If the job terminates before the file appears, an error is printed and the command exits with code 1.</p>"},{"location":"cli/#-remote","title":"<code>--remote</code>","text":"<p>Run <code>aegis submit</code> from a machine that does not have <code>qsub</code> available (e.g., your laptop) by tunnelling commands over SSH:</p> <pre><code>aegis submit --config config.yaml --remote user@aurora.alcf.anl.gov\n</code></pre> <p>ALCF login nodes require a one-time password (OTP). Aegis opens an SSH <code>ControlMaster</code> session that prompts once for the OTP and reuses the connection for all subsequent operations (file copy, <code>qsub</code>, polling). The control socket is cleaned up on exit.</p> <p>Combine with <code>--wait</code> to submit remotely and block until endpoints are ready. The endpoints file is automatically copied to the current working directory:</p> <pre><code>aegis submit --config config.yaml --remote user@aurora.alcf.anl.gov --wait\n</code></pre>"},{"location":"cli/#common-flags-shared-with-launch","title":"Common flags (shared with <code>launch</code>)","text":"Flag Type Description <code>--config</code> <code>str</code> Path to YAML config file <code>--model</code> <code>str</code> HuggingFace model name <code>--instances</code> <code>int</code> Number of vLLM instances to launch <code>--tensor-parallel-size</code> <code>int</code> Number of GPUs per instance <code>--port-start</code> <code>int</code> Base port for each node (incremented for additional instances on the same node) <code>--hf-home</code> <code>str</code> Path to model weights <code>--hf-token</code> <code>str</code> HuggingFace token <code>--model-source</code> <code>str</code> Source path for bcast weight staging <code>--walltime</code> <code>str</code> PBS walltime <code>--queue</code> <code>str</code> PBS queue name <code>--account</code> <code>str</code> PBS account/project <code>--filesystems</code> <code>str</code> PBS filesystem directive <code>--download-weights</code> flag Download model weights from HuggingFace Hub before staging <code>--extra-vllm-args</code> <code>str...</code> Additional arguments passed to <code>vllm serve</code> <code>--registry-port</code> <code>int</code> Port for the service registry HTTP API (default: 8471) <code>--conda-env</code> <code>str</code> Path to a conda-pack tarball to distribute and activate on all nodes <code>--startup-timeout</code> <code>int</code> Seconds to wait for instances to become healthy (default: 600) <code>--endpoints-file</code> <code>str</code> Output path for the endpoints file (default: <code>aegis_endpoints.txt</code>)"},{"location":"cli/#aegis-launch","title":"<code>aegis launch</code>","text":"<p>Launch vLLM instances inside an existing PBS allocation. Stages model weights and starts <code>vllm serve</code> processes on assigned nodes.</p> <pre><code>aegis launch --config config.yaml\n</code></pre>"},{"location":"cli/#launch-specific-flags","title":"Launch-specific flags","text":"Flag Description <code>--skip-staging</code> Skip conda env and weight staging (use when already staged) <p>All common flags listed above are also available.</p>"},{"location":"cli/#aegis-registry","title":"<code>aegis registry</code>","text":"<p>Query the service registry to discover running vLLM instances.</p>"},{"location":"cli/#common-registry-flags","title":"Common registry flags","text":"Flag Type Default Description <code>--registry-host</code> <code>str</code> <code>localhost</code> Registry server host <code>--registry-port</code> <code>int</code> <code>8471</code> Registry server port <code>--format</code> <code>text\\|json</code> <code>text</code> Output format"},{"location":"cli/#aegis-registry-list","title":"<code>aegis registry list</code>","text":"<p>List all registered services.</p> <pre><code>aegis registry list [--type TYPE] [--status STATUS]\n</code></pre> Flag Type Description <code>--type</code> <code>str</code> Filter by service type <code>--status</code> <code>str</code> Filter by status"},{"location":"cli/#aegis-registry-get","title":"<code>aegis registry get</code>","text":"<p>Get a single service by ID.</p> <pre><code>aegis registry get SERVICE_ID\n</code></pre>"},{"location":"cli/#aegis-registry-list-healthy","title":"<code>aegis registry list-healthy</code>","text":"<p>List services that are currently healthy (recent heartbeat).</p> <pre><code>aegis registry list-healthy [--type TYPE] [--timeout SECONDS]\n</code></pre> Flag Type Default Description <code>--type</code> <code>str</code> Filter by service type <code>--timeout</code> <code>int</code> <code>30</code> Heartbeat timeout in seconds"},{"location":"cli/#aegis-registry-count","title":"<code>aegis registry count</code>","text":"<p>Count registered services.</p> <pre><code>aegis registry count [--type TYPE]\n</code></pre> Flag Type Description <code>--type</code> <code>str</code> Filter by service type"},{"location":"cli/#aegis-bench","title":"<code>aegis bench</code>","text":"<p>Benchmark launched vLLM instances using <code>vllm bench serve</code>. Aegis runs benchmarks in parallel across all endpoints via <code>mpiexec</code>, then aggregates results into a CSV summary.</p> <pre><code>aegis bench --model meta-llama/Llama-3.3-70B-Instruct\n</code></pre>"},{"location":"cli/#flags","title":"Flags","text":"Flag Type Default Description <code>--model</code> <code>str</code> required Model name for the benchmark <code>--num-prompts</code> <code>int</code> <code>100</code> Number of prompts per endpoint <code>--endpoints-file</code> <code>str</code> <code>aegis_endpoints.txt</code> Path to endpoints file <code>--output</code> <code>str</code> stdout Path to write CSV results <code>--conda-env</code> <code>str</code> Path to staged conda environment directory <code>--registry-host</code> <code>str</code> <code>localhost</code> Registry server host (use to discover endpoints from registry instead of file) <code>--registry-port</code> <code>int</code> <code>8471</code> Registry server port <code>--format</code> <code>text\\|json</code> <code>text</code> Output format <p>Extra arguments after <code>--</code> are passed through to <code>vllm bench serve</code>:</p> <pre><code>aegis bench --model meta-llama/Llama-3.3-70B-Instruct -- --dataset-name random --random-input-len 512 --random-output-len 128\n</code></pre>"},{"location":"cli/#endpoint-discovery","title":"Endpoint discovery","text":"<p>By default, <code>aegis bench</code> reads endpoints from the file specified by <code>--endpoints-file</code>. If <code>--registry-host</code> is set to something other than <code>localhost</code>, it queries the service registry for healthy endpoints instead.</p>"},{"location":"cli/#conda-environment","title":"Conda environment","text":"<p>If your compute nodes use a staged conda environment for vLLM, pass <code>--conda-env</code> so that each benchmark rank activates the environment before running:</p> <pre><code>aegis bench --model meta-llama/Llama-3.3-70B-Instruct --conda-env /tmp/conda_env\n</code></pre>"},{"location":"cli/#output","title":"Output","text":"<p>Results are printed as CSV to stdout (or to a file with <code>--output</code>). Each row corresponds to one endpoint, with a <code>SUMMARY</code> row showing min/max/mean across all endpoints. Metrics include request throughput, token throughput, TTFT, TPOT, ITL, and more.</p>"},{"location":"cli/#aegis-shutdown","title":"<code>aegis shutdown</code>","text":"<p>Shut down launched vLLM instances and/or cancel PBS jobs. Supports two modes that can be used independently or combined.</p> <pre><code>aegis shutdown\n</code></pre>"},{"location":"cli/#flags_1","title":"Flags","text":"Flag Type Default Description <code>--endpoints-file</code> <code>str</code> <code>aegis_endpoints.txt</code> Path to endpoints file <code>--job-id</code> <code>str</code> PBS job ID to cancel via <code>qdel</code> <code>--remote</code> <code>str</code> Run <code>qdel</code> via SSH on a remote login node (e.g., <code>user@aurora.alcf.anl.gov</code>)"},{"location":"cli/#process-kill-default","title":"Process kill (default)","text":"<p>When the endpoints file exists, <code>aegis shutdown</code> reads it to discover which nodes are running vLLM instances, then uses <code>mpiexec</code> to run <code>pkill -f \"vllm serve\"</code> on each node:</p> <pre><code>aegis shutdown --endpoints-file aegis_endpoints.txt\n</code></pre>"},{"location":"cli/#pbs-job-cancel","title":"PBS job cancel","text":"<p>Use <code>--job-id</code> to cancel a PBS job via <code>qdel</code>, which kills all child processes:</p> <pre><code>aegis shutdown --job-id 12345.aurora-pbs-0001\n</code></pre>"},{"location":"cli/#combined","title":"Combined","text":"<p>Kill processes on nodes first, then cancel the PBS job:</p> <pre><code>aegis shutdown --endpoints-file aegis_endpoints.txt --job-id 12345.aurora-pbs-0001\n</code></pre>"},{"location":"cli/#remote-cancel","title":"Remote cancel","text":"<p>If <code>qdel</code> is not available locally, use <code>--remote</code> to run it via SSH:</p> <pre><code>aegis shutdown --job-id 12345.aurora-pbs-0001 --remote user@aurora.alcf.anl.gov\n</code></pre>"},{"location":"configuration/","title":"Configuration","text":"<p>Aegis uses YAML configuration files. All config values can also be overridden via CLI flags, which take precedence over the config file.</p>"},{"location":"configuration/#single-model-config","title":"Single Model Config","text":"<pre><code>model: meta-llama/Llama-3.3-70B-Instruct\ninstances: 2\ntensor_parallel_size: 6\nport_start: 8000\nhf_home: /tmp/hf_home\nmodel_source: /flare/datasets/model-weights/hub/models--meta-llama--Llama-3.3-70B-Instruct\nwalltime: \"01:00:00\"\naccount: MyProject\nfilesystems: flare:home\nextra_vllm_args:\n  - --max-model-len\n  - \"32768\"\n</code></pre>"},{"location":"configuration/#multi-model-config","title":"Multi-Model Config","text":"<p>Launch different models within a single job allocation. Each model can have its own instance count, tensor-parallel size, weight source, and vLLM arguments. Ports are assigned per node starting from <code>port_start</code>, incrementing only for additional instances on the same node.</p> <pre><code>port_start: 8000\nhf_home: /tmp/hf_home\nwalltime: \"01:00:00\"\naccount: MyProject\nfilesystems: flare:home\n\nmodels:\n  - model: meta-llama/Llama-3.3-70B-Instruct\n    instances: 2\n    tensor_parallel_size: 6\n    model_source: /flare/datasets/model-weights/hub/models--meta-llama--Llama-3.3-70B-Instruct\n    extra_vllm_args:\n      - --max-model-len\n      - \"32768\"\n  - model: meta-llama/Llama-3.1-8B-Instruct\n    instances: 1\n    tensor_parallel_size: 1\n</code></pre>"},{"location":"configuration/#global-settings-aegisconfig","title":"Global Settings (AegisConfig)","text":"Field Type Default Description <code>port_start</code> <code>int</code> <code>8000</code> Base port for each node. Incremented for additional instances on the same node. <code>hf_home</code> <code>str</code> <code>/tmp/hf_home</code> Path used as <code>HF_HOME</code> for model weights. <code>hf_token</code> <code>str</code> <code>None</code> HuggingFace token for gated model access. <code>walltime</code> <code>str</code> <code>01:00:00</code> PBS job walltime. <code>queue</code> <code>str</code> <code>None</code> PBS queue name. <code>account</code> <code>str</code> <code>\"\"</code> PBS account/project. Required for <code>submit</code>. <code>filesystems</code> <code>str</code> <code>flare:home</code> PBS filesystem directive. <code>conda_env</code> <code>str</code> <code>None</code> Path to a conda-pack tarball to distribute and activate on all nodes. <code>registry_port</code> <code>int</code> <code>8471</code> Port for the in-process service registry HTTP API. <code>startup_timeout</code> <code>int</code> <code>600</code> Seconds to wait for instances to become healthy. <code>aegis_env</code> <code>str</code> <code>None</code> Path to a conda environment containing the aegis package. Used by <code>submit</code> to activate the environment in the PBS script. <code>endpoints_file</code> <code>str</code> <code>aegis_endpoints.txt</code> Output path for the file listing healthy instance endpoints. <code>models</code> <code>list</code> <code>[]</code> List of per-model configurations (see below)."},{"location":"configuration/#per-model-settings-modelconfig","title":"Per-Model Settings (ModelConfig)","text":"<p>These fields can appear at the top level (single-model mode) or within entries in the <code>models</code> list (multi-model mode).</p> Field Type Default Description <code>model</code> <code>str</code> <code>\"\"</code> HuggingFace model name (e.g., <code>meta-llama/Llama-3.3-70B-Instruct</code>). <code>instances</code> <code>int</code> <code>1</code> Number of vLLM instances to launch for this model. <code>tensor_parallel_size</code> <code>int</code> <code>1</code> Number of GPUs per instance. <code>model_source</code> <code>str</code> <code>None</code> Source path for MPI broadcast weight staging. <code>download_weights</code> <code>bool</code> <code>false</code> Download model weights from HuggingFace Hub before staging. <code>extra_vllm_args</code> <code>list[str]</code> <code>[]</code> Additional arguments passed to <code>vllm serve</code>."},{"location":"configuration/#computed-properties","title":"Computed Properties","text":"<ul> <li><code>nodes_per_instance</code> \u2014 Number of nodes each instance spans, calculated as <code>ceil(tensor_parallel_size / 12)</code> (12 GPUs per node on Aurora).</li> <li><code>nodes_needed</code> \u2014 Total nodes needed across all models: <code>sum(instances * nodes_per_instance)</code>.</li> </ul>"},{"location":"configuration/#precedence","title":"Precedence","text":"<ol> <li>CLI flags (highest priority)</li> <li>YAML config file</li> <li>Default values (lowest priority)</li> </ol>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>Access to an HPC cluster with PBS (Aurora) or Slurm (Frontier, planned)</li> <li>vLLM (can be installed as part of a conda environment staged by Aegis via the <code>--conda-env</code> option, or available on compute nodes via <code>module load frameworks</code> on Aurora)</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install from the repository root:</p> <pre><code>pip install .\n</code></pre> <p>For development (editable install):</p> <pre><code>pip install -e .\n</code></pre> <p>Verify the installation:</p> <pre><code>aegis --help\n</code></pre>"},{"location":"getting-started/#installing-in-a-conda-environment","title":"Installing in a conda environment","text":"<p>Create a standalone conda environment for Aegis:</p> <pre><code>conda create -n aegis python=3.11 -y\nconda activate aegis\npip install .\n</code></pre> <p>Aegis is a launcher \u2014 it does not require vLLM itself. vLLM only needs to be available on the compute nodes, either via a system module (e.g., <code>module load frameworks</code> on Aurora) or a separate conda environment distributed with <code>--conda-env</code>. See Staging a Conda Environment below.</p>"},{"location":"getting-started/#first-launch","title":"First Launch","text":""},{"location":"getting-started/#1-create-a-config-file","title":"1. Create a config file","text":"<p>Create a file called <code>config.yaml</code>:</p> <pre><code>model: meta-llama/Llama-3.3-70B-Instruct\ninstances: 2\ntensor_parallel_size: 6\nport_start: 8000\nhf_home: /tmp/hf_home\nmodel_source: /flare/datasets/model-weights/hub/models--meta-llama--Llama-3.3-70B-Instruct\nwalltime: \"01:00:00\"\naccount: MyProject\nfilesystems: flare:home\nextra_vllm_args:\n  - --max-model-len\n  - \"32768\"\n</code></pre>"},{"location":"getting-started/#2-preview-the-generated-pbs-script","title":"2. Preview the generated PBS script","text":"<p>Use <code>--dry-run</code> to see what would be submitted without actually submitting:</p> <pre><code>aegis submit --config config.yaml --dry-run\n</code></pre>"},{"location":"getting-started/#3-submit-the-job","title":"3. Submit the job","text":"<pre><code>aegis submit --config config.yaml\n</code></pre> <p>This generates a PBS batch script and submits it via <code>qsub</code>. The job will call <code>aegis launch</code> inside the allocation to stage weights and start vLLM instances.</p>"},{"location":"getting-started/#4-launch-inside-an-existing-allocation","title":"4. Launch inside an existing allocation","text":"<p>If you already have a PBS allocation (e.g., via <code>qsub -I</code>), launch instances directly:</p> <pre><code>aegis launch --config config.yaml\n</code></pre>"},{"location":"getting-started/#5-query-running-instances","title":"5. Query running instances","text":"<p>Once instances are running, use the service registry to discover them:</p> <pre><code>aegis registry list\naegis registry list-healthy\n</code></pre>"},{"location":"getting-started/#6-benchmark-running-instances","title":"6. Benchmark running instances","text":"<p>Once instances are running, benchmark them:</p> <pre><code>aegis bench --model meta-llama/Llama-3.3-70B-Instruct\n</code></pre> <p>This runs <code>vllm bench serve</code> in parallel across all endpoints listed in <code>aegis_endpoints.txt</code> and prints a CSV summary. Write results to a file with <code>--output</code>:</p> <pre><code>aegis bench --model meta-llama/Llama-3.3-70B-Instruct --output results.csv\n</code></pre>"},{"location":"getting-started/#7-shut-down-instances","title":"7. Shut down instances","text":"<p>When you're done, shut down the vLLM processes:</p> <pre><code>aegis shutdown\n</code></pre> <p>This reads <code>aegis_endpoints.txt</code> to find the nodes and kills vLLM processes on each one. To also cancel the PBS job:</p> <pre><code>aegis shutdown --job-id 12345.aurora-pbs-0001\n</code></pre> <p>If you submitted remotely, cancel the job via SSH:</p> <pre><code>aegis shutdown --job-id 12345.aurora-pbs-0001 --remote user@aurora.alcf.anl.gov\n</code></pre>"},{"location":"getting-started/#staging-a-conda-environment","title":"Staging a Conda Environment","text":"<p>If your compute nodes don't have vLLM pre-installed, you can distribute a custom conda environment to all nodes using conda-pack. First, create a tarball from an existing conda environment:</p> <pre><code>conda pack -n my_vllm_env -o my_vllm_env.tar.gz\n</code></pre> <p>Then reference it in your config file:</p> <pre><code>model: meta-llama/Llama-3.3-70B-Instruct\ninstances: 2\ntensor_parallel_size: 6\nconda_env: /path/to/my_vllm_env.tar.gz\nwalltime: \"01:00:00\"\naccount: MyProject\nfilesystems: flare:home\n</code></pre> <p>Or pass it as a CLI flag:</p> <pre><code>aegis submit --config config.yaml --conda-env /path/to/my_vllm_env.tar.gz\n</code></pre> <p>Aegis will broadcast the tarball to all compute nodes and activate the environment before launching vLLM instances.</p>"},{"location":"getting-started/#overriding-config-via-cli","title":"Overriding Config via CLI","text":"<p>All config values can be overridden with CLI flags. CLI flags take precedence over the config file:</p> <pre><code>aegis submit \\\n    --config config.yaml \\\n    --instances 4 \\\n    --walltime 02:00:00 \\\n    --dry-run\n</code></pre> <p>See CLI Reference for all available flags and Configuration for the full config format.</p>"},{"location":"models/","title":"Tested Models","text":"<p>The following models have been tested with Aegis on Aurora.</p>"},{"location":"models/#summary","title":"Summary","text":"Model Parameters TP Size Instances Tested Max Nodes Platform Llama 3.1 8B Instruct 8B 1 up to 1024 1024 Aurora Llama 3.3 70B Instruct 70B 8 up to 1024 1024 Aurora OpenAI gpt-oss-120b 120B 8 up to 1024 1024 Aurora"},{"location":"models/#model-details","title":"Model Details","text":""},{"location":"models/#llama-31-8b-instruct","title":"Llama 3.1 8B Instruct","text":"<ul> <li>Model: <code>meta-llama/Llama-3.1-8B-Instruct</code></li> <li>Tensor parallel size: 1 (fits on a single GPU tile)</li> <li>Nodes per instance: 1</li> <li>Tested at scale: Up to 1024 instances on 1024 nodes</li> </ul> <pre><code>model: meta-llama/Llama-3.1-8B-Instruct\ninstances: 1024\ntensor_parallel_size: 1\n</code></pre>"},{"location":"models/#llama-33-70b-instruct","title":"Llama 3.3 70B Instruct","text":"<ul> <li>Model: <code>meta-llama/Llama-3.3-70B-Instruct</code></li> <li>Tensor parallel size: 8</li> <li>Nodes per instance: 1</li> <li>Tested at scale: Up to 1024 instances on 1024 nodes</li> </ul> <pre><code>model: meta-llama/Llama-3.3-70B-Instruct\ninstances: 1024\ntensor_parallel_size: 8\nmodel_source: /flare/datasets/model-weights/hub/models--meta-llama--Llama-3.3-70B-Instruct\nextra_vllm_args:\n  - --max-model-len\n  - \"32768\"\n</code></pre>"},{"location":"models/#openai-gpt-oss-120b","title":"OpenAI gpt-oss-120b","text":"<ul> <li>Model: <code>OpenAI/gpt-oss-120b</code></li> <li>Tensor parallel size: 8</li> <li>Nodes per instance: 1</li> <li>Tested at scale: Up to 1024 instances on 1024 nodes</li> </ul> <pre><code>model: OpenAI/gpt-oss-120b\ninstances: 1024\ntensor_parallel_size: 8\n</code></pre>"},{"location":"models/#scalability","title":"Scalability","text":"<p>All three models have been tested on Aurora at scales up to 1024 nodes. Aegis handles the orchestration of weight staging and instance launch across all nodes automatically via MPI broadcast and PBS job management.</p> <p>Key observations:</p> <ul> <li>Weight staging via MPI broadcast scales efficiently to 1024 nodes</li> <li>The service registry tracks instance health across all nodes</li> <li>Port assignment is handled automatically per node, with <code>port_start</code> incrementing for additional instances on the same node</li> </ul>"},{"location":"roadmap/","title":"Roadmap","text":"<p>Planned development for Aegis.</p>"},{"location":"roadmap/#frontier-support","title":"Frontier Support","text":"<ul> <li>Add Slurm job generation and submission (alongside existing PBS support)</li> <li>Integrate ROCm/Apptainer-based vLLM launch workflow for AMD MI250X GPUs</li> <li>Validate weight staging on Frontier's Burst Buffer storage</li> </ul>"},{"location":"roadmap/#additional-models","title":"Additional Models","text":"<ul> <li>Expand tested model list beyond Llama 3 and gpt-oss-120b</li> <li>Test and document multi-node configurations for models exceeding single-node GPU memory</li> <li>Add support for quantized model variants</li> </ul>"},{"location":"roadmap/#larger-scale-testing","title":"Larger Scale Testing","text":"<ul> <li>Test beyond 1024 nodes on Aurora</li> <li>Benchmark instance startup time and weight staging throughput at scale</li> <li>Evaluate service registry performance under high instance counts</li> </ul>"},{"location":"roadmap/#usability","title":"Usability","text":"<ul> <li>Improved error reporting and diagnostics for failed instances</li> <li>Support for custom vLLM server configurations beyond <code>extra_vllm_args</code></li> <li>Configuration validation with actionable error messages</li> </ul>"},{"location":"Aurora/","title":"Inference with vLLM on Aurora","text":""},{"location":"Aurora/#load-frameworks-module-with-xpu-enabled-vllm-installation","title":"Load frameworks module with XPU-enabled vLLM installation","text":"<pre><code>module load frameworks\nexport NUMEXPR_MAX_THREADS=208 # number of CPU threads on Aurora node\nexport CCL_PROCESS_LAUNCHER=torchrun\nvllm --version\n</code></pre>"},{"location":"Aurora/#example-output-from-aurora-compute-node","title":"Example output from Aurora compute node","text":"<pre><code>raffenet@x4311c1s4b0n0:~&gt; module load frameworks\n(/opt/aurora/25.190.0/frameworks/aurora_frameworks-2025.2.0) raffenet@x4311c1s4b0n0:~&gt; vllm --version\n[W1106 16:55:04.833350173 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -&gt; Tensor(a!)\n    registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/pytorch_2p8_rel_07_18_2025/pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/pytorch_2p8_rel_07_18_2025/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37\n       new kernel: registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/ipex_2.8.10_xpu_rel_08_18_2025/intel-extension-for-pytorch/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())\n[2025-11-06 16:55:04,586] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to xpu (auto detect)\n[2025-11-06 16:55:06,888] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\nINFO 11-06 16:55:07 [__init__.py:241] Automatically detected platform xpu.\n0.10.1rc2.dev189+ge2db1164a.xpu\n</code></pre>"},{"location":"Aurora/#model-weights","title":"Model Weights","text":"<p>Model weights for commonly used open-weight models are downloaded and available in the <code>/flare/datasets/model-weights/hub</code> directory on Aurora. However accessing these weights from more than a single GPU can be very time-consuming. For multiple GPU scenarios, We recommend reading the weights once from the parallel filesystem and distributing them to node local storage with MPI using [tools/bcast.c]. <pre><code>export HF_HOME=\"/tmp/hf_home\"\n</code></pre></p> <p>Some models hosted on Hugging Face may be gated, requiring additional authentication. To access these gated models, you will need a Hugging Face authentication token. <pre><code>export HF_TOKEN=\"YOUR_HF_TOKEN\"\n</code></pre></p>"},{"location":"Aurora/#serve-small-models","title":"Serve Small Models","text":"<p>For small models that fit within a single tile's memory (64 GB), no additional configuration is required to serve the model. The model will run on a single tile. Models with fewer than 7 billion parameters typically fit within a single tile.</p>"},{"location":"Aurora/#using-single-gpu-tile","title":"Using Single GPU Tile","text":"<p>The following command serves <code>meta-llama/Llama-2-7b-chat-hf</code> on a single tile of a single node: <pre><code>module load frameworks\nexport NUMEXPR_MAX_THREADS=208\nexport CCL_PROCESS_LAUNCHER=torchrun\nexport HF_HOME=/flare/datasets/model-weights\nexport HF_TOKEN=&lt;your_token&gt;\nvllm serve meta-llama/Llama-2-7b-chat-hf\n</code></pre></p> Click for example output <pre><code>(/opt/aurora/25.190.0/frameworks/aurora_frameworks-2025.2.0) raffenet@x4516c6s1b0n0:~&gt; vllm serve meta-llama/Llama-2-7b-chat-hf\n\n[W1106 18:40:40.729878143 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -&gt; Tensor(a!)\n    registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/pytorch_2p8_rel_07_18_2025/pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/pytorch_2p8_rel_07_18_2025/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37\n       new kernel: registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/ipex_2.8.10_xpu_rel_08_18_2025/intel-extension-for-pytorch/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())\n[2025-11-06 18:40:40,292] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to xpu (auto detect)\n[2025-11-06 18:40:42,596] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\nINFO 11-06 18:40:42 [__init__.py:241] Automatically detected platform xpu.\n(APIServer pid=206609) INFO 11-06 18:40:44 [api_server.py:1873] vLLM API server version 0.10.1rc2.dev189+ge2db1164a\n(APIServer pid=206609) INFO 11-06 18:40:44 [utils.py:326] non-default args: {'model_tag': 'meta-llama/Llama-2-7b-chat-hf', 'model': 'meta-llama/Llama-2-7b-chat-hf'}\n(APIServer pid=206609) Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/flare/datasets/model-weights/hub/models--meta-llama--Llama-2-7b-chat-hf/.no_exist/f5db02db724555f92da89c216ac04704f23d4590/preprocessor_config.json'\n(APIServer pid=206609) ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/flare/datasets/model-weights/hub/models--meta-llama--Llama-2-7b-chat-hf/.no_exist/f5db02db724555f92da89c216ac04704f23d4590/preprocessor_config.json'\n(APIServer pid=206609) INFO 11-06 18:40:53 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n(APIServer pid=206609) `torch_dtype` is deprecated! Use `dtype` instead!\n(APIServer pid=206609) INFO 11-06 18:40:53 [__init__.py:1786] Using max model len 4096\n(APIServer pid=206609) INFO 11-06 18:40:53 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n(APIServer pid=206609) INFO 11-06 18:40:53 [xpu.py:113] [XPU] CUDA graph is not supported on XPU, disabling cudagraphs.\n[W1106 18:40:59.700780685 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -&gt; Tensor(a!)\n    registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/pytorch_2p8_rel_07_18_2025/pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/pytorch_2p8_rel_07_18_2025/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37\n       new kernel: registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/ipex_2.8.10_xpu_rel_08_18_2025/intel-extension-for-pytorch/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())\n[2025-11-06 18:40:59,263] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to xpu (auto detect)\n[2025-11-06 18:41:01,390] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\nINFO 11-06 18:41:01 [__init__.py:241] Automatically detected platform xpu.\n(EngineCore_0 pid=207574) INFO 11-06 18:41:02 [core.py:644] Waiting for init message from front-end.\n(EngineCore_0 pid=207574) INFO 11-06 18:41:02 [core.py:74] Initializing a V1 LLM engine (v0.10.1rc2.dev189+ge2db1164a) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=xpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n(EngineCore_0 pid=207574) INFO 11-06 18:41:03 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n2025:11:06-18:41:03:(207574) |CCL_WARN| value of CCL_ATL_TRANSPORT changed to be ofi (default:mpi)\n2025:11:06-18:41:03:(207574) |CCL_WARN| value of CCL_OP_SYNC changed to be 1 (default:0)\n2025:11:06-18:41:03:(207574) |CCL_WARN| value of CCL_PROCESS_LAUNCHER changed to be torchrun (default:hydra)\n(EngineCore_0 pid=207574) INFO 11-06 18:41:03 [gpu_model_runner.py:1964] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n(EngineCore_0 pid=207574) INFO 11-06 18:41:03 [gpu_model_runner.py:1996] Loading model from scratch...\n(EngineCore_0 pid=207574) INFO 11-06 18:41:03 [xpu.py:45] Using Flash Attention backend on V1 engine.\n(EngineCore_0 pid=207574) INFO 11-06 18:41:04 [weight_utils.py:294] Using model weights format ['*.safetensors']\n(EngineCore_0 pid=207574) Ignored error while writing commit hash to /flare/datasets/model-weights/hub/models--meta-llama--Llama-2-7b-chat-hf/refs/main: [Errno 13] Permission denied: '/flare/datasets/model-weights/hub/models--meta-llama--Llama-2-7b-chat-hf/refs/main'.\n(EngineCore_0 pid=207574) WARNING:huggingface_hub._snapshot_download:Ignored error while writing commit hash to /flare/datasets/model-weights/hub/models--meta-llama--Llama-2-7b-chat-hf/refs/main: [Errno 13] Permission denied: '/flare/datasets/model-weights/hub/models--meta-llama--Llama-2-7b-chat-hf/refs/main'.\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00&lt;?, ?it/s]\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:01&lt;00:01,  1.01s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:04&lt;00:00,  2.23s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:04&lt;00:00,  2.05s/it]\n(EngineCore_0 pid=207574)\n(EngineCore_0 pid=207574) INFO 11-06 18:41:08 [default_loader.py:267] Loading weights took 4.35 seconds\n(EngineCore_0 pid=207574) INFO 11-06 18:41:08 [gpu_model_runner.py:2018] Model loading took 12.5533 GiB and 5.050575 seconds\n(EngineCore_0 pid=207574) INFO 11-06 18:41:09 [xpu_worker.py:104] Before memory profiling run, total GPU memory: 65520.00 MB, model load takes 12870.74 MB, free gpu memory is 52435.44 MB.\n(EngineCore_0 pid=207574) INFO 11-06 18:41:10 [xpu_worker.py:139] After memory profiling run, peak memory usage is 13552.69 MB,torch mem is 12870.74 MB, non-torch mem is 426.94 MB, free gpu memory is 51896.32 MB.\n(EngineCore_0 pid=207574) INFO 11-06 18:41:10 [kv_cache_utils.py:849] GPU KV cache size: 90,816 tokens\n(EngineCore_0 pid=207574) INFO 11-06 18:41:10 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 22.17x\n(EngineCore_0 pid=207574) WARNING 11-06 18:41:10 [_logger.py:68] Skipping CUDA graph capture. To turn on CUDA graph capture, ensure `cudagraph_mode` was not manually set to `NONE`\n(EngineCore_0 pid=207574) INFO 11-06 18:41:10 [core.py:215] init engine (profile, create kv cache, warmup model) took 1.44 seconds\n(APIServer pid=206609) INFO 11-06 18:41:10 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 1419\n(APIServer pid=206609) INFO 11-06 18:41:10 [async_llm.py:165] Torch profiler disabled. AsyncLLM CPU traces will not be collected.\n(APIServer pid=206609) INFO 11-06 18:41:10 [api_server.py:1679] Supported_tasks: ['generate']\n(APIServer pid=206609) WARNING 11-06 18:41:10 [logger.py:71] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n(APIServer pid=206609) INFO 11-06 18:41:10 [serving_responses.py:124] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n(APIServer pid=206609) INFO 11-06 18:41:10 [serving_chat.py:135] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n(APIServer pid=206609) INFO 11-06 18:41:10 [serving_completion.py:77] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n(APIServer pid=206609) INFO 11-06 18:41:10 [api_server.py:1948] Starting vLLM API server 0 on http://0.0.0.0:8000\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:36] Available routes are:\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /docs, Methods: GET, HEAD\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /redoc, Methods: GET, HEAD\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /health, Methods: GET\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /load, Methods: GET\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /ping, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /ping, Methods: GET\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /tokenize, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /detokenize, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/models, Methods: GET\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /version, Methods: GET\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/responses, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/chat/completions, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/completions, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/embeddings, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /pooling, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /classify, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /score, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/score, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/audio/translations, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /rerank, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/rerank, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v2/rerank, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /invocations, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /metrics, Methods: GET\n(APIServer pid=206609) INFO:     Started server process [206609]\n(APIServer pid=206609) INFO:     Waiting for application startup.\n(APIServer pid=206609) INFO:     Application startup complete.\n</code></pre>"},{"location":"Aurora/#serve-medium-models-using-multiple-gpu-tiles","title":"Serve Medium Models Using Multiple GPU Tiles","text":"<p>The following script demonstrates how to serve the <code>meta-llama/Llama-3.3-70B-Instruct</code> model across 8 tiles on a single node using tensor parallelism:</p> <pre><code>module load frameworks\nexport NUMEXPR_MAX_THREADS=208\nexport CCL_PROCESS_LAUNCHER=torchrun\nexport HF_HOME=/tmp/hf_home\nexport HF_TOKEN=&lt;your_token&gt;\n# copy weights to /tmp for faster loading\nmpiexec -ppn 1 ./bcast /flare/datasets/model-weights/hub/models--meta-llama--Llama-3.3-70B-Instruct /tmp/hf_home/hub\nvllm serve meta-llama/Llama-3.3-70B-Instruct --tensor-parallel-size 8\n</code></pre>"},{"location":"Aurora/#serve-large-models-using-multiple-nodes","title":"Serve Large Models Using Multiple Nodes","text":"<p>TBD</p>"},{"location":"Frontier/","title":"Inference with vLLM on Frontier","text":"<p>Until there are proven working instructions for running vLLM natively on Frontier nodes, we recommend using the ROCm vLLM Docker images published in the vLLM DockerHub organization.</p>"},{"location":"Frontier/#build-and-run-rocm-vllm-image","title":"Build and run ROCm vLLM image","text":"<pre><code>apptainer build vllm-openai-rocm.sif docker://vllm/vllm-openai-rocm\napptainer shell vllm-openai-rocm.sif\nvllm --version\n</code></pre>"},{"location":"Frontier/#example-output-from-frontier-compute-node","title":"Example output from Frontier compute node","text":"<pre><code>Apptainer&gt; vllm --version\n0.11.1rc6.dev141+g0b8e871e5.rocm700\n</code></pre>"},{"location":"Frontier/#access-model-weights","title":"Access Model Weights","text":"<p>To my knowledge there are no common models stored on the Frontier filesystem for community use. It may be a good idea to do so for whatever ModCon allocation we receive. In any case, we recommend using Frontier's Burst Buffer storage for staging model weights.</p> <p>When downloading models from Hugging Face you will need to set your http proxy settings for outbound access. Be aware that some gated models also require additional authentication. To access these gated models, you will need a Hugging Face authentication token.</p> <pre><code>export all_proxy=socks://proxy.ccs.ornl.gov:3128/\nexport ftp_proxy=ftp://proxy.ccs.ornl.gov:3128/\nexport http_proxy=http://proxy.ccs.ornl.gov:3128/\nexport https_proxy=http://proxy.ccs.ornl.gov:3128/\nexport no_proxy='localhost,127.0.0.0/8,*.ccs.ornl.gov'\nexport HF_TOKEN=&lt;your_token&gt;\n</code></pre>"},{"location":"Frontier/#serve-small-models","title":"Serve Small Models","text":"<p>For small models that fit within a single GPU's memory (64 GB), no additional configuration is required to serve the model. The defaultconfiguration ensures the model is run on a single tile without the need for distributed setup. Models with fewer than 7 billion parameters typically fit within a single tile.</p>"},{"location":"Frontier/#using-single-tile","title":"Using Single Tile","text":"<p>The following command serves <code>meta-llama/Llama-2-7b-chat-hf</code> on a single tile of a single node: <pre><code>vllm serve meta-llama/Llama-2-7b-chat-hf\n</code></pre></p> Click for example output <pre><code>&lt;tbd&gt;\n</code></pre>"},{"location":"Frontier/#using-multiple-tiles","title":"Using Multiple Tiles","text":"<p>To utilize multiple tiles for larger models (<code>TP&gt;1</code>), a more advanced setup is necessary. First, configure a Ray cluster. <pre><code>export VLLM_HOST_IP=$(hostname -I | awk '{print $1}')\nunset ROCM_VISIBLE_DEVICES # vLLM throws an error if it sees this envvar\nray start --head --node-ip-address=$VLLM_HOST_IP --num-cpus=128 --num-gpus=8 &amp;\n</code></pre></p> <p>The following script demonstrates how to serve the <code>meta-llama/Llama-2-7b-chat-hf</code> model across 8 tiles on a single node:</p> <pre><code>export VLLM_HOST_IP=$(hostname -I | awk '{print $1}')\nunset ROCM_VISIBLE_DEVICES\nray start --head --node-ip-address=$VLLM_HOST_IP --num-cpus=128 --num-gpus=8 &amp;\nvllm serve meta-llama/Llama-2-7b-chat-hf --port 8000 --tensor-parallel-size 8 --trust-remote-code\n</code></pre>"},{"location":"Frontier/#serve-medium-models","title":"Serve Medium Models","text":""},{"location":"Frontier/#using-single-node","title":"Using Single Node","text":"<p>The following script demonstrates how to serve <code>meta-llama/Llama-3.3-70B-Instruct</code> on 8 tiles on a single node. Models with up to 70 billion parameters can usually fit within a single node, utilizing multiple tiles.</p> <pre><code>export VLLM_HOST_IP=$(hostname -I | awk '{print $1}')\nunset ROCM_VISIBLE_DEVICES # vLLM throws an error if it sees this envvar\nray start --head --node-ip-address=$VLLM_HOST_IP --num-cpus=128 --num-gpus=8 &amp;\nvllm serve meta-llama/Llama-3.3-70B-Instruct --tensor-parallel-size 8 --trust-remote-code --max-model-len 32768\n</code></pre>"},{"location":"Frontier/#serve-large-models","title":"Serve Large Models","text":""},{"location":"Frontier/#using-multiple-nodes","title":"Using Multiple Nodes","text":"<p>coming soon...</p>"},{"location":"platforms/aurora/","title":"Aurora","text":"<p>Aurora is an Intel Data Center GPU Max (Ponte Vecchio) system at the Argonne Leadership Computing Facility, managed with PBS.</p>"},{"location":"platforms/aurora/#example-config","title":"Example Config","text":"<p>Serve Llama 3.3 70B on Aurora with tensor parallelism across 8 GPU tiles:</p> <pre><code>model: meta-llama/Llama-3.3-70B-Instruct\ninstances: 2\ntensor_parallel_size: 8\nmodel_source: /flare/datasets/model-weights/hub/models--meta-llama--Llama-3.3-70B-Instruct\nwalltime: \"01:00:00\"\naccount: MyProject\nfilesystems: flare:home\nextra_vllm_args:\n  - --max-model-len\n  - \"32768\"\n</code></pre>"},{"location":"platforms/aurora/#submitting-jobs","title":"Submitting Jobs","text":"<p>Submit from an Aurora login node:</p> <pre><code>aegis submit --config config.yaml\n</code></pre> <p>Submit from your laptop via SSH:</p> <pre><code>aegis submit --config config.yaml --remote user@aurora.alcf.anl.gov\n</code></pre> <p>Submit and wait for endpoints to be ready:</p> <pre><code>aegis submit --config config.yaml --wait\n</code></pre> <p>These flags can be combined \u2014 see CLI Reference for the full list.</p>"},{"location":"platforms/aurora/#vllm-availability","title":"vLLM Availability","text":"<p>vLLM is pre-installed on Aurora compute nodes via <code>module load frameworks</code>. Alternatively, distribute a custom environment with the <code>--conda-env</code> option (see Getting Started).</p>"},{"location":"platforms/frontier/","title":"Frontier","text":"<p>Frontier is an AMD Instinct MI250X system at the Oak Ridge Leadership Computing Facility, managed with Slurm.</p> <p>Warning</p> <p>Slurm support in Aegis is planned but not yet implemented.</p> <p>When Frontier support is added, Aegis will generate and submit Slurm batch scripts (via <code>sbatch</code>) in the same way it currently handles PBS jobs on Aurora. Configuration, weight staging, and instance orchestration will work through the same <code>aegis submit</code> interface.</p>"}]}