{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Aegis: HPC LLM Instance Launcher","text":"<p>Aegis automates launching configurable numbers of vLLM inference instances on HPC clusters. It handles PBS job generation, model weight staging via MPI broadcast, and per-instance orchestration.</p> <p>Currently targets Aurora (PBS). Frontier/Slurm support is planned.</p>"},{"location":"#quick-install","title":"Quick Install","text":"<pre><code>pip install .\n</code></pre> <p>For development:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"#how-it-works","title":"How It Works","text":"<ol> <li><code>aegis submit</code> renders a PBS batch script from a Jinja2 template and submits it via <code>qsub</code>. The generated job script calls <code>aegis launch</code> inside the allocation.</li> <li><code>aegis launch</code> runs inside a PBS allocation. It optionally stages model weights to local storage using MPI broadcast, then launches one <code>vllm serve</code> process per instance on the assigned nodes.</li> </ol>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>src/aegis/\n\u251c\u2500\u2500 cli.py              # CLI entry point (argparse)\n\u251c\u2500\u2500 config.py           # Config file loading + merging with CLI args\n\u251c\u2500\u2500 scheduler.py        # PBS job generation and submission\n\u251c\u2500\u2500 launcher.py         # Core orchestration: stage weights, launch instances\n\u2514\u2500\u2500 templates/\n    \u251c\u2500\u2500 pbs_job.sh.j2   # Jinja2 template for PBS batch script\n    \u2514\u2500\u2500 instance.sh.j2  # Jinja2 template for per-node vLLM launch script\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started \u2014 Installation and first launch walkthrough</li> <li>CLI Reference \u2014 All commands and flags</li> <li>Configuration \u2014 YAML config format and all options</li> <li>Platforms \u2014 Platform-specific setup guides</li> </ul>"},{"location":"cli/","title":"CLI Reference","text":"<p>Aegis provides three subcommands: <code>submit</code>, <code>launch</code>, and <code>registry</code>.</p>"},{"location":"cli/#aegis-submit","title":"<code>aegis submit</code>","text":"<p>Generate and submit a PBS batch job.</p> <pre><code>aegis submit --config config.yaml\n</code></pre>"},{"location":"cli/#submit-specific-flags","title":"Submit-specific flags","text":"Flag Description <code>--dry-run</code> Print the generated PBS script without submitting"},{"location":"cli/#common-flags-shared-with-launch","title":"Common flags (shared with <code>launch</code>)","text":"Flag Type Description <code>--config</code> <code>str</code> Path to YAML config file <code>--model</code> <code>str</code> HuggingFace model name <code>--instances</code> <code>int</code> Number of vLLM instances to launch <code>--tensor-parallel-size</code> <code>int</code> Number of GPUs per instance <code>--port-start</code> <code>int</code> Base port for each node (incremented for additional instances on the same node) <code>--hf-home</code> <code>str</code> Path to model weights <code>--hf-token</code> <code>str</code> HuggingFace token <code>--model-source</code> <code>str</code> Source path for bcast weight staging <code>--walltime</code> <code>str</code> PBS walltime <code>--queue</code> <code>str</code> PBS queue name <code>--account</code> <code>str</code> PBS account/project <code>--filesystems</code> <code>str</code> PBS filesystem directive <code>--download-weights</code> flag Download model weights from HuggingFace Hub before staging <code>--extra-vllm-args</code> <code>str...</code> Additional arguments passed to <code>vllm serve</code> <code>--registry-port</code> <code>int</code> Port for the service registry HTTP API (default: 8471) <code>--conda-env</code> <code>str</code> Path to a conda-pack tarball to distribute and activate on all nodes <code>--startup-timeout</code> <code>int</code> Seconds to wait for instances to become healthy (default: 600)"},{"location":"cli/#aegis-launch","title":"<code>aegis launch</code>","text":"<p>Launch vLLM instances inside an existing PBS allocation. Stages model weights and starts <code>vllm serve</code> processes on assigned nodes.</p> <pre><code>aegis launch --config config.yaml\n</code></pre>"},{"location":"cli/#launch-specific-flags","title":"Launch-specific flags","text":"Flag Description <code>--skip-staging</code> Skip conda env and weight staging (use when already staged) <p>All common flags listed above are also available.</p>"},{"location":"cli/#aegis-registry","title":"<code>aegis registry</code>","text":"<p>Query the service registry to discover running vLLM instances.</p>"},{"location":"cli/#common-registry-flags","title":"Common registry flags","text":"Flag Type Default Description <code>--registry-host</code> <code>str</code> <code>localhost</code> Registry server host <code>--registry-port</code> <code>int</code> <code>8471</code> Registry server port <code>--format</code> <code>text\\|json</code> <code>text</code> Output format"},{"location":"cli/#aegis-registry-list","title":"<code>aegis registry list</code>","text":"<p>List all registered services.</p> <pre><code>aegis registry list [--type TYPE] [--status STATUS]\n</code></pre> Flag Type Description <code>--type</code> <code>str</code> Filter by service type <code>--status</code> <code>str</code> Filter by status"},{"location":"cli/#aegis-registry-get","title":"<code>aegis registry get</code>","text":"<p>Get a single service by ID.</p> <pre><code>aegis registry get SERVICE_ID\n</code></pre>"},{"location":"cli/#aegis-registry-list-healthy","title":"<code>aegis registry list-healthy</code>","text":"<p>List services that are currently healthy (recent heartbeat).</p> <pre><code>aegis registry list-healthy [--type TYPE] [--timeout SECONDS]\n</code></pre> Flag Type Default Description <code>--type</code> <code>str</code> Filter by service type <code>--timeout</code> <code>int</code> <code>30</code> Heartbeat timeout in seconds"},{"location":"cli/#aegis-registry-count","title":"<code>aegis registry count</code>","text":"<p>Count registered services.</p> <pre><code>aegis registry count [--type TYPE]\n</code></pre> Flag Type Description <code>--type</code> <code>str</code> Filter by service type"},{"location":"configuration/","title":"Configuration","text":"<p>Aegis uses YAML configuration files. All config values can also be overridden via CLI flags, which take precedence over the config file.</p>"},{"location":"configuration/#single-model-config","title":"Single Model Config","text":"<pre><code>model: meta-llama/Llama-3.3-70B-Instruct\ninstances: 2\ntensor_parallel_size: 6\nport_start: 8000\nhf_home: /tmp/hf_home\nmodel_source: /flare/datasets/model-weights/hub/models--meta-llama--Llama-3.3-70B-Instruct\nwalltime: \"01:00:00\"\naccount: MyProject\nfilesystems: flare:home\nextra_vllm_args:\n  - --max-model-len\n  - \"32768\"\n</code></pre>"},{"location":"configuration/#multi-model-config","title":"Multi-Model Config","text":"<p>Launch different models within a single job allocation. Each model can have its own instance count, tensor-parallel size, weight source, and vLLM arguments. Ports are assigned per node starting from <code>port_start</code>, incrementing only for additional instances on the same node.</p> <pre><code>port_start: 8000\nhf_home: /tmp/hf_home\nwalltime: \"01:00:00\"\naccount: MyProject\nfilesystems: flare:home\n\nmodels:\n  - model: meta-llama/Llama-3.3-70B-Instruct\n    instances: 2\n    tensor_parallel_size: 6\n    model_source: /flare/datasets/model-weights/hub/models--meta-llama--Llama-3.3-70B-Instruct\n    extra_vllm_args:\n      - --max-model-len\n      - \"32768\"\n  - model: meta-llama/Llama-3.1-8B-Instruct\n    instances: 1\n    tensor_parallel_size: 1\n</code></pre>"},{"location":"configuration/#global-settings-aegisconfig","title":"Global Settings (AegisConfig)","text":"Field Type Default Description <code>port_start</code> <code>int</code> <code>8000</code> Base port for each node. Incremented for additional instances on the same node. <code>hf_home</code> <code>str</code> <code>/tmp/hf_home</code> Path used as <code>HF_HOME</code> for model weights. <code>hf_token</code> <code>str</code> <code>None</code> HuggingFace token for gated model access. <code>walltime</code> <code>str</code> <code>01:00:00</code> PBS job walltime. <code>queue</code> <code>str</code> <code>None</code> PBS queue name. <code>account</code> <code>str</code> <code>\"\"</code> PBS account/project. Required for <code>submit</code>. <code>filesystems</code> <code>str</code> <code>flare:home</code> PBS filesystem directive. <code>conda_env</code> <code>str</code> <code>None</code> Path to a conda-pack tarball to distribute and activate on all nodes. <code>registry_port</code> <code>int</code> <code>8471</code> Port for the in-process service registry HTTP API. <code>startup_timeout</code> <code>int</code> <code>600</code> Seconds to wait for instances to become healthy. <code>models</code> <code>list</code> <code>[]</code> List of per-model configurations (see below)."},{"location":"configuration/#per-model-settings-modelconfig","title":"Per-Model Settings (ModelConfig)","text":"<p>These fields can appear at the top level (single-model mode) or within entries in the <code>models</code> list (multi-model mode).</p> Field Type Default Description <code>model</code> <code>str</code> <code>\"\"</code> HuggingFace model name (e.g., <code>meta-llama/Llama-3.3-70B-Instruct</code>). <code>instances</code> <code>int</code> <code>1</code> Number of vLLM instances to launch for this model. <code>tensor_parallel_size</code> <code>int</code> <code>1</code> Number of GPUs per instance. <code>model_source</code> <code>str</code> <code>None</code> Source path for MPI broadcast weight staging. <code>download_weights</code> <code>bool</code> <code>false</code> Download model weights from HuggingFace Hub before staging. <code>extra_vllm_args</code> <code>list[str]</code> <code>[]</code> Additional arguments passed to <code>vllm serve</code>."},{"location":"configuration/#computed-properties","title":"Computed Properties","text":"<ul> <li><code>nodes_per_instance</code> \u2014 Number of nodes each instance spans, calculated as <code>ceil(tensor_parallel_size / 12)</code> (12 GPUs per node on Aurora).</li> <li><code>nodes_needed</code> \u2014 Total nodes needed across all models: <code>sum(instances * nodes_per_instance)</code>.</li> </ul>"},{"location":"configuration/#precedence","title":"Precedence","text":"<ol> <li>CLI flags (highest priority)</li> <li>YAML config file</li> <li>Default values (lowest priority)</li> </ol>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>Access to an HPC cluster with PBS (Aurora) or Slurm (Frontier, planned)</li> <li>vLLM (can be installed as part of a conda environment staged by Aegis via the <code>--conda-env</code> option, or available on compute nodes via <code>module load frameworks</code> on Aurora)</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install from the repository root:</p> <pre><code>pip install .\n</code></pre> <p>For development (editable install):</p> <pre><code>pip install -e .\n</code></pre> <p>Verify the installation:</p> <pre><code>aegis --help\n</code></pre>"},{"location":"getting-started/#first-launch","title":"First Launch","text":""},{"location":"getting-started/#1-create-a-config-file","title":"1. Create a config file","text":"<p>Create a file called <code>config.yaml</code>:</p> <pre><code>model: meta-llama/Llama-3.3-70B-Instruct\ninstances: 2\ntensor_parallel_size: 6\nport_start: 8000\nhf_home: /tmp/hf_home\nmodel_source: /flare/datasets/model-weights/hub/models--meta-llama--Llama-3.3-70B-Instruct\nwalltime: \"01:00:00\"\naccount: MyProject\nfilesystems: flare:home\nextra_vllm_args:\n  - --max-model-len\n  - \"32768\"\n</code></pre>"},{"location":"getting-started/#2-preview-the-generated-pbs-script","title":"2. Preview the generated PBS script","text":"<p>Use <code>--dry-run</code> to see what would be submitted without actually submitting:</p> <pre><code>aegis submit --config config.yaml --dry-run\n</code></pre>"},{"location":"getting-started/#3-submit-the-job","title":"3. Submit the job","text":"<pre><code>aegis submit --config config.yaml\n</code></pre> <p>This generates a PBS batch script and submits it via <code>qsub</code>. The job will call <code>aegis launch</code> inside the allocation to stage weights and start vLLM instances.</p>"},{"location":"getting-started/#4-launch-inside-an-existing-allocation","title":"4. Launch inside an existing allocation","text":"<p>If you already have a PBS allocation (e.g., via <code>qsub -I</code>), launch instances directly:</p> <pre><code>aegis launch --config config.yaml\n</code></pre>"},{"location":"getting-started/#5-query-running-instances","title":"5. Query running instances","text":"<p>Once instances are running, use the service registry to discover them:</p> <pre><code>aegis registry list\naegis registry list-healthy\n</code></pre>"},{"location":"getting-started/#overriding-config-via-cli","title":"Overriding Config via CLI","text":"<p>All config values can be overridden with CLI flags. CLI flags take precedence over the config file:</p> <pre><code>aegis submit \\\n    --config config.yaml \\\n    --instances 4 \\\n    --walltime 02:00:00 \\\n    --dry-run\n</code></pre> <p>See CLI Reference for all available flags and Configuration for the full config format.</p>"},{"location":"models/","title":"Tested Models","text":"<p>The following models have been tested with Aegis on Aurora.</p>"},{"location":"models/#summary","title":"Summary","text":"Model Parameters TP Size Instances Tested Max Nodes Platform Llama 3.1 8B Instruct 8B 1 up to 1024 1024 Aurora Llama 3.3 70B Instruct 70B 6 up to 1024 1024 Aurora OpenAI gpt-oss-120b 120B 12 up to 1024 1024 Aurora"},{"location":"models/#model-details","title":"Model Details","text":""},{"location":"models/#llama-31-8b-instruct","title":"Llama 3.1 8B Instruct","text":"<ul> <li>Model: <code>meta-llama/Llama-3.1-8B-Instruct</code></li> <li>Tensor parallel size: 1 (fits on a single GPU tile)</li> <li>Nodes per instance: 1</li> <li>Tested at scale: Up to 1024 instances on 1024 nodes</li> </ul> <pre><code>model: meta-llama/Llama-3.1-8B-Instruct\ninstances: 1024\ntensor_parallel_size: 1\n</code></pre>"},{"location":"models/#llama-33-70b-instruct","title":"Llama 3.3 70B Instruct","text":"<ul> <li>Model: <code>meta-llama/Llama-3.3-70B-Instruct</code></li> <li>Tensor parallel size: 6</li> <li>Nodes per instance: 1</li> <li>Tested at scale: Up to 1024 instances on 1024 nodes</li> </ul> <pre><code>model: meta-llama/Llama-3.3-70B-Instruct\ninstances: 1024\ntensor_parallel_size: 6\nmodel_source: /flare/datasets/model-weights/hub/models--meta-llama--Llama-3.3-70B-Instruct\nextra_vllm_args:\n  - --max-model-len\n  - \"32768\"\n</code></pre>"},{"location":"models/#openai-gpt-oss-120b","title":"OpenAI gpt-oss-120b","text":"<ul> <li>Model: <code>OpenAI/gpt-oss-120b</code></li> <li>Tensor parallel size: 12 (full node)</li> <li>Nodes per instance: 1</li> <li>Tested at scale: Up to 1024 instances on 1024 nodes</li> </ul> <pre><code>model: OpenAI/gpt-oss-120b\ninstances: 1024\ntensor_parallel_size: 12\n</code></pre>"},{"location":"models/#scalability","title":"Scalability","text":"<p>All three models have been tested on Aurora at scales up to 1024 nodes. Aegis handles the orchestration of weight staging and instance launch across all nodes automatically via MPI broadcast and PBS job management.</p> <p>Key observations:</p> <ul> <li>Weight staging via MPI broadcast scales efficiently to 1024 nodes</li> <li>The service registry tracks instance health across all nodes</li> <li>Port assignment is handled automatically per node, with <code>port_start</code> incrementing for additional instances on the same node</li> </ul>"},{"location":"roadmap/","title":"Roadmap","text":"<p>Planned development for Aegis.</p>"},{"location":"roadmap/#frontier-support","title":"Frontier Support","text":"<ul> <li>Add Slurm job generation and submission (alongside existing PBS support)</li> <li>Integrate ROCm/Apptainer-based vLLM launch workflow for AMD MI250X GPUs</li> <li>Validate weight staging on Frontier's Burst Buffer storage</li> </ul>"},{"location":"roadmap/#additional-models","title":"Additional Models","text":"<ul> <li>Expand tested model list beyond Llama 3 and gpt-oss-120b</li> <li>Test and document multi-node configurations for models exceeding single-node GPU memory</li> <li>Add support for quantized model variants</li> </ul>"},{"location":"roadmap/#larger-scale-testing","title":"Larger Scale Testing","text":"<ul> <li>Test beyond 1024 nodes on Aurora</li> <li>Benchmark instance startup time and weight staging throughput at scale</li> <li>Evaluate service registry performance under high instance counts</li> </ul>"},{"location":"roadmap/#usability","title":"Usability","text":"<ul> <li>Improved error reporting and diagnostics for failed instances</li> <li>Support for custom vLLM server configurations beyond <code>extra_vllm_args</code></li> <li>Configuration validation with actionable error messages</li> </ul>"},{"location":"Aurora/","title":"Inference with vLLM on Aurora","text":""},{"location":"Aurora/#load-frameworks-module-with-xpu-enabled-vllm-installation","title":"Load frameworks module with XPU-enabled vLLM installation","text":"<pre><code>module load frameworks\nexport NUMEXPR_MAX_THREADS=208 # number of CPU threads on Aurora node\nexport CCL_PROCESS_LAUNCHER=torchrun\nvllm --version\n</code></pre>"},{"location":"Aurora/#example-output-from-aurora-compute-node","title":"Example output from Aurora compute node","text":"<pre><code>raffenet@x4311c1s4b0n0:~&gt; module load frameworks\n(/opt/aurora/25.190.0/frameworks/aurora_frameworks-2025.2.0) raffenet@x4311c1s4b0n0:~&gt; vllm --version\n[W1106 16:55:04.833350173 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -&gt; Tensor(a!)\n    registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/pytorch_2p8_rel_07_18_2025/pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/pytorch_2p8_rel_07_18_2025/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37\n       new kernel: registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/ipex_2.8.10_xpu_rel_08_18_2025/intel-extension-for-pytorch/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())\n[2025-11-06 16:55:04,586] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to xpu (auto detect)\n[2025-11-06 16:55:06,888] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\nINFO 11-06 16:55:07 [__init__.py:241] Automatically detected platform xpu.\n0.10.1rc2.dev189+ge2db1164a.xpu\n</code></pre>"},{"location":"Aurora/#model-weights","title":"Model Weights","text":"<p>Model weights for commonly used open-weight models are downloaded and available in the <code>/flare/datasets/model-weights/hub</code> directory on Aurora. However accessing these weights from more than a single GPU can be very time-consuming. For multiple GPU scenarios, We recommend reading the weights once from the parallel filesystem and distributing them to node local storage with MPI using [tools/bcast.c]. <pre><code>export HF_HOME=\"/tmp/hf_home\"\n</code></pre></p> <p>Some models hosted on Hugging Face may be gated, requiring additional authentication. To access these gated models, you will need a Hugging Face authentication token. <pre><code>export HF_TOKEN=\"YOUR_HF_TOKEN\"\n</code></pre></p>"},{"location":"Aurora/#serve-small-models","title":"Serve Small Models","text":"<p>For small models that fit within a single tile's memory (64 GB), no additional configuration is required to serve the model. The model will run on a single tile. Models with fewer than 7 billion parameters typically fit within a single tile.</p>"},{"location":"Aurora/#using-single-gpu-tile","title":"Using Single GPU Tile","text":"<p>The following command serves <code>meta-llama/Llama-2-7b-chat-hf</code> on a single tile of a single node: <pre><code>module load frameworks\nexport NUMEXPR_MAX_THREADS=208\nexport CCL_PROCESS_LAUNCHER=torchrun\nexport HF_HOME=/flare/datasets/model-weights\nexport HF_TOKEN=&lt;your_token&gt;\nvllm serve meta-llama/Llama-2-7b-chat-hf\n</code></pre></p> Click for example output <pre><code>(/opt/aurora/25.190.0/frameworks/aurora_frameworks-2025.2.0) raffenet@x4516c6s1b0n0:~&gt; vllm serve meta-llama/Llama-2-7b-chat-hf\n\n[W1106 18:40:40.729878143 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -&gt; Tensor(a!)\n    registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/pytorch_2p8_rel_07_18_2025/pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/pytorch_2p8_rel_07_18_2025/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37\n       new kernel: registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/ipex_2.8.10_xpu_rel_08_18_2025/intel-extension-for-pytorch/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())\n[2025-11-06 18:40:40,292] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to xpu (auto detect)\n[2025-11-06 18:40:42,596] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\nINFO 11-06 18:40:42 [__init__.py:241] Automatically detected platform xpu.\n(APIServer pid=206609) INFO 11-06 18:40:44 [api_server.py:1873] vLLM API server version 0.10.1rc2.dev189+ge2db1164a\n(APIServer pid=206609) INFO 11-06 18:40:44 [utils.py:326] non-default args: {'model_tag': 'meta-llama/Llama-2-7b-chat-hf', 'model': 'meta-llama/Llama-2-7b-chat-hf'}\n(APIServer pid=206609) Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/flare/datasets/model-weights/hub/models--meta-llama--Llama-2-7b-chat-hf/.no_exist/f5db02db724555f92da89c216ac04704f23d4590/preprocessor_config.json'\n(APIServer pid=206609) ERROR:huggingface_hub.file_download:Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/flare/datasets/model-weights/hub/models--meta-llama--Llama-2-7b-chat-hf/.no_exist/f5db02db724555f92da89c216ac04704f23d4590/preprocessor_config.json'\n(APIServer pid=206609) INFO 11-06 18:40:53 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n(APIServer pid=206609) `torch_dtype` is deprecated! Use `dtype` instead!\n(APIServer pid=206609) INFO 11-06 18:40:53 [__init__.py:1786] Using max model len 4096\n(APIServer pid=206609) INFO 11-06 18:40:53 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n(APIServer pid=206609) INFO 11-06 18:40:53 [xpu.py:113] [XPU] CUDA graph is not supported on XPU, disabling cudagraphs.\n[W1106 18:40:59.700780685 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.\n  Overriding a previously registered kernel for the same operator and the same dispatch key\n  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -&gt; Tensor(a!)\n    registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/pytorch_2p8_rel_07_18_2025/pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n  dispatch key: XPU\n  previous kernel: registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/pytorch_2p8_rel_07_18_2025/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37\n       new kernel: registered at /lus/tegu/projects/datasets/software/wheelforge/repositories/ipex_2.8.10_xpu_rel_08_18_2025/intel-extension-for-pytorch/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())\n[2025-11-06 18:40:59,263] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to xpu (auto detect)\n[2025-11-06 18:41:01,390] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\nINFO 11-06 18:41:01 [__init__.py:241] Automatically detected platform xpu.\n(EngineCore_0 pid=207574) INFO 11-06 18:41:02 [core.py:644] Waiting for init message from front-end.\n(EngineCore_0 pid=207574) INFO 11-06 18:41:02 [core.py:74] Initializing a V1 LLM engine (v0.10.1rc2.dev189+ge2db1164a) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=xpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n(EngineCore_0 pid=207574) INFO 11-06 18:41:03 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n2025:11:06-18:41:03:(207574) |CCL_WARN| value of CCL_ATL_TRANSPORT changed to be ofi (default:mpi)\n2025:11:06-18:41:03:(207574) |CCL_WARN| value of CCL_OP_SYNC changed to be 1 (default:0)\n2025:11:06-18:41:03:(207574) |CCL_WARN| value of CCL_PROCESS_LAUNCHER changed to be torchrun (default:hydra)\n(EngineCore_0 pid=207574) INFO 11-06 18:41:03 [gpu_model_runner.py:1964] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n(EngineCore_0 pid=207574) INFO 11-06 18:41:03 [gpu_model_runner.py:1996] Loading model from scratch...\n(EngineCore_0 pid=207574) INFO 11-06 18:41:03 [xpu.py:45] Using Flash Attention backend on V1 engine.\n(EngineCore_0 pid=207574) INFO 11-06 18:41:04 [weight_utils.py:294] Using model weights format ['*.safetensors']\n(EngineCore_0 pid=207574) Ignored error while writing commit hash to /flare/datasets/model-weights/hub/models--meta-llama--Llama-2-7b-chat-hf/refs/main: [Errno 13] Permission denied: '/flare/datasets/model-weights/hub/models--meta-llama--Llama-2-7b-chat-hf/refs/main'.\n(EngineCore_0 pid=207574) WARNING:huggingface_hub._snapshot_download:Ignored error while writing commit hash to /flare/datasets/model-weights/hub/models--meta-llama--Llama-2-7b-chat-hf/refs/main: [Errno 13] Permission denied: '/flare/datasets/model-weights/hub/models--meta-llama--Llama-2-7b-chat-hf/refs/main'.\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00&lt;?, ?it/s]\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:01&lt;00:01,  1.01s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:04&lt;00:00,  2.23s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:04&lt;00:00,  2.05s/it]\n(EngineCore_0 pid=207574)\n(EngineCore_0 pid=207574) INFO 11-06 18:41:08 [default_loader.py:267] Loading weights took 4.35 seconds\n(EngineCore_0 pid=207574) INFO 11-06 18:41:08 [gpu_model_runner.py:2018] Model loading took 12.5533 GiB and 5.050575 seconds\n(EngineCore_0 pid=207574) INFO 11-06 18:41:09 [xpu_worker.py:104] Before memory profiling run, total GPU memory: 65520.00 MB, model load takes 12870.74 MB, free gpu memory is 52435.44 MB.\n(EngineCore_0 pid=207574) INFO 11-06 18:41:10 [xpu_worker.py:139] After memory profiling run, peak memory usage is 13552.69 MB,torch mem is 12870.74 MB, non-torch mem is 426.94 MB, free gpu memory is 51896.32 MB.\n(EngineCore_0 pid=207574) INFO 11-06 18:41:10 [kv_cache_utils.py:849] GPU KV cache size: 90,816 tokens\n(EngineCore_0 pid=207574) INFO 11-06 18:41:10 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 22.17x\n(EngineCore_0 pid=207574) WARNING 11-06 18:41:10 [_logger.py:68] Skipping CUDA graph capture. To turn on CUDA graph capture, ensure `cudagraph_mode` was not manually set to `NONE`\n(EngineCore_0 pid=207574) INFO 11-06 18:41:10 [core.py:215] init engine (profile, create kv cache, warmup model) took 1.44 seconds\n(APIServer pid=206609) INFO 11-06 18:41:10 [loggers.py:142] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 1419\n(APIServer pid=206609) INFO 11-06 18:41:10 [async_llm.py:165] Torch profiler disabled. AsyncLLM CPU traces will not be collected.\n(APIServer pid=206609) INFO 11-06 18:41:10 [api_server.py:1679] Supported_tasks: ['generate']\n(APIServer pid=206609) WARNING 11-06 18:41:10 [logger.py:71] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n(APIServer pid=206609) INFO 11-06 18:41:10 [serving_responses.py:124] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n(APIServer pid=206609) INFO 11-06 18:41:10 [serving_chat.py:135] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n(APIServer pid=206609) INFO 11-06 18:41:10 [serving_completion.py:77] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n(APIServer pid=206609) INFO 11-06 18:41:10 [api_server.py:1948] Starting vLLM API server 0 on http://0.0.0.0:8000\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:36] Available routes are:\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /docs, Methods: GET, HEAD\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /redoc, Methods: GET, HEAD\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /health, Methods: GET\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /load, Methods: GET\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /ping, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /ping, Methods: GET\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /tokenize, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /detokenize, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/models, Methods: GET\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /version, Methods: GET\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/responses, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/chat/completions, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/completions, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/embeddings, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /pooling, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /classify, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /score, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/score, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/audio/translations, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /rerank, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v1/rerank, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /v2/rerank, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /invocations, Methods: POST\n(APIServer pid=206609) INFO 11-06 18:41:10 [launcher.py:44] Route: /metrics, Methods: GET\n(APIServer pid=206609) INFO:     Started server process [206609]\n(APIServer pid=206609) INFO:     Waiting for application startup.\n(APIServer pid=206609) INFO:     Application startup complete.\n</code></pre>"},{"location":"Aurora/#serve-medium-models-using-multiple-gpu-tiles","title":"Serve Medium Models Using Multiple GPU Tiles","text":"<p>The following script demonstrates how to serve the <code>meta-llama/Llama-3.3-70B-Instruct</code> model across 8 tiles on a single node using tensor parallelism:</p> <pre><code>module load frameworks\nexport NUMEXPR_MAX_THREADS=208\nexport CCL_PROCESS_LAUNCHER=torchrun\nexport HF_HOME=/tmp/hf_home\nexport HF_TOKEN=&lt;your_token&gt;\n# copy weights to /tmp for faster loading\nmpiexec -ppn 1 ./bcast /flare/datasets/model-weights/hub/models--meta-llama--Llama-3.3-70B-Instruct /tmp/hf_home/hub\nvllm serve meta-llama/Llama-3.3-70B-Instruct --tensor-parallel-size 8\n</code></pre>"},{"location":"Aurora/#serve-large-models-using-multiple-nodes","title":"Serve Large Models Using Multiple Nodes","text":"<p>TBD</p>"},{"location":"Frontier/","title":"Inference with vLLM on Frontier","text":"<p>Until there are proven working instructions for running vLLM natively on Frontier nodes, we recommend using the ROCm vLLM Docker images published in the vLLM DockerHub organization.</p>"},{"location":"Frontier/#build-and-run-rocm-vllm-image","title":"Build and run ROCm vLLM image","text":"<pre><code>apptainer build vllm-openai-rocm.sif docker://vllm/vllm-openai-rocm\napptainer shell vllm-openai-rocm.sif\nvllm --version\n</code></pre>"},{"location":"Frontier/#example-output-from-frontier-compute-node","title":"Example output from Frontier compute node","text":"<pre><code>Apptainer&gt; vllm --version\n0.11.1rc6.dev141+g0b8e871e5.rocm700\n</code></pre>"},{"location":"Frontier/#access-model-weights","title":"Access Model Weights","text":"<p>To my knowledge there are no common models stored on the Frontier filesystem for community use. It may be a good idea to do so for whatever ModCon allocation we receive. In any case, we recommend using Frontier's Burst Buffer storage for staging model weights.</p> <p>When downloading models from Hugging Face you will need to set your http proxy settings for outbound access. Be aware that some gated models also require additional authentication. To access these gated models, you will need a Hugging Face authentication token.</p> <pre><code>export all_proxy=socks://proxy.ccs.ornl.gov:3128/\nexport ftp_proxy=ftp://proxy.ccs.ornl.gov:3128/\nexport http_proxy=http://proxy.ccs.ornl.gov:3128/\nexport https_proxy=http://proxy.ccs.ornl.gov:3128/\nexport no_proxy='localhost,127.0.0.0/8,*.ccs.ornl.gov'\nexport HF_TOKEN=&lt;your_token&gt;\n</code></pre>"},{"location":"Frontier/#serve-small-models","title":"Serve Small Models","text":"<p>For small models that fit within a single GPU's memory (64 GB), no additional configuration is required to serve the model. The defaultconfiguration ensures the model is run on a single tile without the need for distributed setup. Models with fewer than 7 billion parameters typically fit within a single tile.</p>"},{"location":"Frontier/#using-single-tile","title":"Using Single Tile","text":"<p>The following command serves <code>meta-llama/Llama-2-7b-chat-hf</code> on a single tile of a single node: <pre><code>vllm serve meta-llama/Llama-2-7b-chat-hf\n</code></pre></p> Click for example output <pre><code>&lt;tbd&gt;\n</code></pre>"},{"location":"Frontier/#using-multiple-tiles","title":"Using Multiple Tiles","text":"<p>To utilize multiple tiles for larger models (<code>TP&gt;1</code>), a more advanced setup is necessary. First, configure a Ray cluster. <pre><code>export VLLM_HOST_IP=$(hostname -I | awk '{print $1}')\nunset ROCM_VISIBLE_DEVICES # vLLM throws an error if it sees this envvar\nray start --head --node-ip-address=$VLLM_HOST_IP --num-cpus=128 --num-gpus=8 &amp;\n</code></pre></p> <p>The following script demonstrates how to serve the <code>meta-llama/Llama-2-7b-chat-hf</code> model across 8 tiles on a single node:</p> <pre><code>export VLLM_HOST_IP=$(hostname -I | awk '{print $1}')\nunset ROCM_VISIBLE_DEVICES\nray start --head --node-ip-address=$VLLM_HOST_IP --num-cpus=128 --num-gpus=8 &amp;\nvllm serve meta-llama/Llama-2-7b-chat-hf --port 8000 --tensor-parallel-size 8 --trust-remote-code\n</code></pre>"},{"location":"Frontier/#serve-medium-models","title":"Serve Medium Models","text":""},{"location":"Frontier/#using-single-node","title":"Using Single Node","text":"<p>The following script demonstrates how to serve <code>meta-llama/Llama-3.3-70B-Instruct</code> on 8 tiles on a single node. Models with up to 70 billion parameters can usually fit within a single node, utilizing multiple tiles.</p> <pre><code>export VLLM_HOST_IP=$(hostname -I | awk '{print $1}')\nunset ROCM_VISIBLE_DEVICES # vLLM throws an error if it sees this envvar\nray start --head --node-ip-address=$VLLM_HOST_IP --num-cpus=128 --num-gpus=8 &amp;\nvllm serve meta-llama/Llama-3.3-70B-Instruct --tensor-parallel-size 8 --trust-remote-code --max-model-len 32768\n</code></pre>"},{"location":"Frontier/#serve-large-models","title":"Serve Large Models","text":""},{"location":"Frontier/#using-multiple-nodes","title":"Using Multiple Nodes","text":"<p>coming soon...</p>"},{"location":"platforms/aurora/","title":"Aurora","text":"<p>Aurora is an Intel Data Center GPU Max (Ponte Vecchio) system at the Argonne Leadership Computing Facility, managed with PBS.</p>"},{"location":"platforms/aurora/#environment-setup","title":"Environment Setup","text":"<p>Load the frameworks module which includes an XPU-enabled vLLM installation:</p> <pre><code>module load frameworks\nexport NUMEXPR_MAX_THREADS=208  # number of CPU threads on Aurora node\nexport CCL_PROCESS_LAUNCHER=torchrun\nvllm --version\n</code></pre>"},{"location":"platforms/aurora/#model-weights","title":"Model Weights","text":"<p>Model weights for commonly used open-weight models are available in <code>/flare/datasets/model-weights/hub</code> on Aurora. However, accessing these weights from more than a single GPU can be very slow. For multi-GPU scenarios, read the weights once from the parallel filesystem and distribute them to node-local storage with MPI broadcast:</p> <pre><code>export HF_HOME=\"/tmp/hf_home\"\nmpiexec -ppn 1 ./bcast /flare/datasets/model-weights/hub/models--meta-llama--Llama-3.3-70B-Instruct /tmp/hf_home/hub\n</code></pre> <p>Note</p> <p>Some models hosted on Hugging Face are gated and require a Hugging Face authentication token:</p> <pre><code>export HF_TOKEN=\"YOUR_HF_TOKEN\"\n</code></pre>"},{"location":"platforms/aurora/#serving-models","title":"Serving Models","text":""},{"location":"platforms/aurora/#small-models-single-gpu-tile","title":"Small Models (Single GPU Tile)","text":"<p>Models with fewer than 7 billion parameters typically fit within a single tile's 64 GB memory. No additional configuration is required:</p> <pre><code>module load frameworks\nexport NUMEXPR_MAX_THREADS=208\nexport CCL_PROCESS_LAUNCHER=torchrun\nexport HF_HOME=/flare/datasets/model-weights\nexport HF_TOKEN=&lt;your_token&gt;\nvllm serve meta-llama/Llama-2-7b-chat-hf\n</code></pre>"},{"location":"platforms/aurora/#medium-models-multiple-gpu-tiles","title":"Medium Models (Multiple GPU Tiles)","text":"<p>For models up to ~70B parameters, use tensor parallelism across tiles on a single node:</p> <pre><code>module load frameworks\nexport NUMEXPR_MAX_THREADS=208\nexport CCL_PROCESS_LAUNCHER=torchrun\nexport HF_HOME=/tmp/hf_home\nexport HF_TOKEN=&lt;your_token&gt;\n# Stage weights to local storage\nmpiexec -ppn 1 ./bcast /flare/datasets/model-weights/hub/models--meta-llama--Llama-3.3-70B-Instruct /tmp/hf_home/hub\nvllm serve meta-llama/Llama-3.3-70B-Instruct --tensor-parallel-size 8\n</code></pre>"},{"location":"platforms/aurora/#large-models-multiple-nodes","title":"Large Models (Multiple Nodes)","text":"<p>For models that exceed single-node capacity, Aegis handles multi-node orchestration automatically. Configure <code>tensor_parallel_size</code> to exceed the GPUs on a single node (12 on Aurora) and Aegis will allocate the necessary nodes:</p> <pre><code>model: large-model/name\ntensor_parallel_size: 24  # spans 2 nodes\ninstances: 1\n</code></pre>"},{"location":"platforms/aurora/#using-aegis-on-aurora","title":"Using Aegis on Aurora","text":"<p>With Aegis, the environment setup, weight staging, and instance orchestration are handled automatically. Create a config file and submit:</p> <pre><code>aegis submit --config config.yaml\n</code></pre> <p>See Getting Started for a full walkthrough.</p>"},{"location":"platforms/frontier/","title":"Frontier","text":"<p>Frontier is an AMD Instinct MI250X system at the Oak Ridge Leadership Computing Facility, managed with Slurm.</p> <p>Warning</p> <p>Frontier support in Aegis is planned but not yet implemented. The instructions below cover manual vLLM setup on Frontier nodes.</p>"},{"location":"platforms/frontier/#environment-setup","title":"Environment Setup","text":"<p>Until native vLLM support is confirmed on Frontier, use the ROCm vLLM Docker images via Apptainer:</p> <pre><code>apptainer build vllm-openai-rocm.sif docker://vllm/vllm-openai-rocm\napptainer shell vllm-openai-rocm.sif\nvllm --version\n</code></pre>"},{"location":"platforms/frontier/#model-weights","title":"Model Weights","text":"<p>Set HTTP proxy settings for outbound access when downloading models from Hugging Face:</p> <pre><code>export all_proxy=socks://proxy.ccs.ornl.gov:3128/\nexport ftp_proxy=ftp://proxy.ccs.ornl.gov:3128/\nexport http_proxy=http://proxy.ccs.ornl.gov:3128/\nexport https_proxy=http://proxy.ccs.ornl.gov:3128/\nexport no_proxy='localhost,127.0.0.0/8,*.ccs.ornl.gov'\n</code></pre> <p>Note</p> <p>Some models hosted on Hugging Face are gated and require a Hugging Face authentication token:</p> <pre><code>export HF_TOKEN=&lt;your_token&gt;\n</code></pre> <p>We recommend using Frontier's Burst Buffer storage for staging model weights.</p>"},{"location":"platforms/frontier/#serving-models","title":"Serving Models","text":""},{"location":"platforms/frontier/#small-models-single-gpu","title":"Small Models (Single GPU)","text":"<p>Models with fewer than 7 billion parameters typically fit within a single GPU's 64 GB memory:</p> <pre><code>vllm serve meta-llama/Llama-2-7b-chat-hf\n</code></pre>"},{"location":"platforms/frontier/#medium-models-multiple-gpus-single-node","title":"Medium Models (Multiple GPUs, Single Node)","text":"<p>For tensor parallelism across multiple GPUs (<code>TP&gt;1</code>), first configure a Ray cluster:</p> <pre><code>export VLLM_HOST_IP=$(hostname -I | awk '{print $1}')\nunset ROCM_VISIBLE_DEVICES  # vLLM throws an error if it sees this envvar\nray start --head --node-ip-address=$VLLM_HOST_IP --num-cpus=128 --num-gpus=8 &amp;\n</code></pre> <p>Then serve the model:</p> <pre><code>vllm serve meta-llama/Llama-3.3-70B-Instruct \\\n    --tensor-parallel-size 8 \\\n    --trust-remote-code \\\n    --max-model-len 32768\n</code></pre>"},{"location":"platforms/frontier/#large-models-multiple-nodes","title":"Large Models (Multiple Nodes)","text":"<p>Multi-node serving on Frontier is not yet documented. This will be updated as Aegis adds Slurm/Frontier support.</p>"}]}