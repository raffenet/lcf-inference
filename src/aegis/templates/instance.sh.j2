#!/bin/bash -l

export PYTHONNOUSERSITE=1

export HTTP_PROXY=http://proxy.alcf.anl.gov:3128
export HTTPS_PROXY=http://proxy.alcf.anl.gov:3128
export http_proxy=http://proxy.alcf.anl.gov:3128
export https_proxy=http://proxy.alcf.anl.gov:3128
export no_proxy=localhost,127.0.0.1

module load pti-gpu
module load hdf5

{% if conda_env %}
mkdir -p /tmp/conda_env && tar -xzf /tmp/{{ conda_env.split('/')[-1] }} -C /tmp/conda_env
set --
source /tmp/conda_env/bin/activate
conda-unpack
export LD_LIBRARY_PATH=/tmp/conda_env/lib/python3.12/site-packages/intel_extension_for_pytorch/lib:/tmp/conda_env/lib:/tmp/conda_env/lib/python3.12/site-packages/torch/lib:${LD_LIBRARY_PATH}:/usr/lib64
export ZE_FLAT_DEVICE_HIERARCHY=FLAT
unset ONEAPI_DEVICE_SELECTOR
export CCL_PROCESS_LAUNCHER=None
export VLLM_WORKER_MULTIPROC_METHOD=spawn
export FI_MR_CACHE_MONITOR=userfaultfd
export TOKENIZERS_PARALLELISM=false
export OCL_ICD_SO="/opt/aurora/25.190.0/oneapi/2025.2/lib/libintelocl.so"
export OCL_ICD_FILENAMES="/opt/aurora/25.190.0/oneapi/2025.2/lib/libintelocl.so"
export VLLM_DISABLE_SINKS=1
export VLLM_CACHE_ROOT="/tmp/hf_home/vllm_cache"
export HF_DATASETS_CACHE="/tmp/hf_home"
export HF_MODULES_CACHE="/tmp/hf_home"
export HF_HUB_OFFLINE=1
{% elif apptainer_image %}
module load apptainer
LIBFABRIC_PREFIX=$(pkg-config libfabric --variable=prefix)
export LD_LIBRARY_PATH=${LIBFABRIC_PREFIX}/lib64:${LD_LIBRARY_PATH}
export ZE_FLAT_DEVICE_HIERARCHY=FLAT
unset ONEAPI_DEVICE_SELECTOR
export CCL_PROCESS_LAUNCHER=None
export VLLM_WORKER_MULTIPROC_METHOD=spawn
export FI_MR_CACHE_MONITOR=userfaultfd
export TOKENIZERS_PARALLELISM=false
export VLLM_DISABLE_SINKS=1
export VLLM_CACHE_ROOT="/tmp/hf_home/vllm_cache"
export HF_DATASETS_CACHE="/tmp/hf_home"
export HF_MODULES_CACHE="/tmp/hf_home"
export HF_HUB_OFFLINE=1
{% else %}
module load frameworks
export CCL_PROCESS_LAUNCHER=torchrun
{% endif %}
export TMPDIR=/tmp
export VLLM_HOST_IP=$(getent hosts $(hostname).hsn.cm.aurora.alcf.anl.gov | awk '{ print $1 }' | tr ' ' '\n' | sort | head -n 1)

if [ -z "${HF_TOKEN:-}" ]; then
    echo "Warning: HF_TOKEN not set. Gated models will fail to load." >&2
fi
export HF_HOME="{{ hf_home }}"

{% if apptainer_image %}
numactl --cpunodebind=0 --membind=0 apptainer exec --bind ${LIBFABRIC_PREFIX} /tmp/{{ apptainer_image.split('/')[-1] }} vllm serve {{ model }} \
{% else %}
numactl --cpunodebind=0 --membind=0 vllm serve {{ model }} \
{% endif -%}
    --dtype bfloat16 \
    --tensor-parallel-size {{ tensor_parallel_size }} \
    --port {{ port }} \
    --enforce-eager \
    --distributed-executor-backend mp \
    --trust-remote-code \
{% for arg in extra_vllm_args %}    {{ arg }} \
{% endfor %}

exit 0
