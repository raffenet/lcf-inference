#!/bin/bash -l

export HTTP_PROXY=http://proxy.alcf.anl.gov:3128
export HTTPS_PROXY=http://proxy.alcf.anl.gov:3128
export http_proxy=http://proxy.alcf.anl.gov:3128
export https_proxy=http://proxy.alcf.anl.gov:3128
export no_proxy=localhost,127.0.0.1

{% if conda_env %}
mkdir -p /tmp/conda_env && tar -xzf /tmp/{{ conda_env.split('/')[-1] }} -C /tmp/conda_env
source /tmp/conda_env/bin/activate
conda-unpack
export ZE_FLAT_DEVICE_HIERARCHY=FLAT
export VLLM_WORKER_MULTIPROC_METHOD=spawn
export FI_MR_CACHE_MONITOR=userfaultfd
export TORCH_COMPILE_DISABLE=1
export OMP_NUM_THREADS=52
unset ONEAPI_DEVICE_SELECTOR
export CCL_PROCESS_LAUNCHER=None
export TOKENIZERS_PARALLELISM=false
export OCL_ICD_SO="/opt/aurora/25.190.0/oneapi/2025.2/lib/libintelocl.so"
export VLLM_DISABLE_SINKS=1
{% else %}
module load frameworks
export CCL_PROCESS_LAUNCHER=torchrun
{% endif %}
export NUMEXPR_MAX_THREADS=208
export TMPDIR=/tmp

if [ -z "${HF_TOKEN:-}" ]; then
    echo "Error: HF_TOKEN not set."
    exit 1
fi
export HF_HOME="{{ hf_home }}"

vllm serve {{ model }} \
    --tensor-parallel-size {{ tensor_parallel_size }} \
    --port {{ port }} \
{% for arg in extra_vllm_args %}    {{ arg }} \
{% endfor %}

exit 0
